# ğŸ§  PromptCache Playground

## Table of contents
* [General info](#general-info)
* [Technologies](#technologies)
* [Setup](#setup).
* [Directory Structure](#directory-structure)
* [How It Works](#how-it-works)
* [Running the App](#running-the-app)

## General info
<b>ğŸ§  What is Prompt Caching?</b>

Prompt caching is a technique used to store and reuse LLM responses for previously seen prompts, avoiding redundant computation and reducing response latency.

Instead of sending the same prompt repeatedly to the LLM:
<ul>
    <li>The system first checks a cache (in this case, an SQLite database)</li>
    <li>If the prompt was seen before, the cached response is returned instantly</li>
    <li>Otherwise, the LLM is queried, and the new result is stored in the cache for future use</li>
    <li>Otherwise, the LLM is queried, and the new result is stored in the cache for future use</li>
</ul>

This project uses LangChainâ€™s caching system to manage cached responses efficiently with:
<ul>
    <li>SQLiteCache: a lightweight local cache for development/testing</li>
    <li>Cache lookup happens transparently before every prompt execution</li>
    <li>Latency-based detection (< 100ms) is used to infer HIT/MISS in real time</li>
</ul>

âœ… Benefits of Prompt Caching<br>
âš¡ Speed: Cached responses are returned almost instantly

ğŸ§  Efficiency: Reduces repeated LLM compute calls

ğŸ’µ Cost-saving: (In API-based setups) prevents redundant billing

ğŸ” Reproducibility: Same prompt always gives same cached result (unless cleared)

A local, fully offline LangChain + Ollama-powered app to demonstrate prompt caching, visualize cache hits/misses, and benchmark prompt latency â€” with a real-time UI built using Streamlit.

---

## ğŸš€ Features

- âš¡ **Prompt Caching via LangChain + SQLite**
- ğŸ§  **Local LLM via Ollama** (e.g., Mistral, LLaMA3)
- ğŸ“Š Real-time **cache HIT/MISS detection**
- ğŸ“ **Prompt logs** (CSV) with latency + timestamps
- ğŸ§¹ Flush cache + logs instantly
- ğŸ–¥ï¸ Beautiful and minimal **Streamlit UI**
- ğŸ” Fully local, **no API keys or internet required**

---

## ğŸ“¸ Demo

![App Demo](assets/demo/caching1.PNG)
![App Demo](assets/demo/caching2.PNG)
![App Demo](assets/demo/caching3.PNG)
![App Demo](assets/demo/caching4.PNG)

## Technologies
* GenAI
* LangChain
* Ollama
* Streamlit
* SQLite
* Python

## Setup
### Installation 
* Download and install Ollama from https://ollama.com/
* run commands: 
    ollama pull mistral
* pip install -r requirements.txt

## Directory structure
promptcache-playground/

â”œâ”€â”€ app.py                  # Streamlit UI

â”œâ”€â”€ prompt_runner.py        # Core logic (LLM, caching, logging)

â”œâ”€â”€ cache_log.csv           # Prompt logs (auto-created)

â”œâ”€â”€ .cache/                 # LangChain SQLite cache (auto-created)

## ğŸ› ï¸ How It Works
<b>ğŸ” Prompt Caching via LangChain</b>
<ul>
    <li>Uses SQLiteCache from LangChain</li>
    <li>When a prompt is run:</li>
    <ul>
        <li>If cached: fetch from SQLite</li>
        <li>If not: LLM is invoked, and response cached</li>
    </ul>
    <li>LLM used: Ollama(model="mistral") (can be changed)</li>
</ul>

<b>ğŸ•µï¸ HIT/MISS Detection</b>
<ul>
    <li>Uses latency threshold (100ms) to infer if cache was hit</li>
    
</ul>

<b>ğŸ“ Logging</b>
All prompt runs are logged in cache_log.csv

## â–¶ï¸ Running the App
Start your local LLM (e.g., Mistral): 

ollama run mistral

Launch the Streamlit UI:

streamlit run app.py

