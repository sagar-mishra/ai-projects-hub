# ğŸ§  PromptCache Playground

## Table of contents
* [General info](#general-info)
* [Technologies](#technologies)
* [Setup](#setup).
* [Directory Structure](#directory-structure)
* [How It Works](#how-it-works)
* [Running the App](#running-the-app)

## General info
A local, fully offline LangChain + Ollama-powered app to demonstrate prompt caching, visualize cache hits/misses, and benchmark prompt latency â€” with a real-time UI built using Streamlit.

---

## ğŸš€ Features

- âš¡ **Prompt Caching via LangChain + SQLite**
- ğŸ§  **Local LLM via Ollama** (e.g., Mistral, LLaMA3)
- ğŸ“Š Real-time **cache HIT/MISS detection**
- ğŸ“ **Prompt logs** (CSV) with latency + timestamps
- ğŸ§¹ Flush cache + logs instantly
- ğŸ–¥ï¸ Beautiful and minimal **Streamlit UI**
- ğŸ” Fully local, **no API keys or internet required**

---

## ğŸ“¸ Demo

![App Demo](assets/demo/caching1.PNG)
![App Demo](assets/demo/caching2.PNG)
![App Demo](assets/demo/caching3.PNG)
![App Demo](assets/demo/caching4.PNG)

## Technologies
* GenAI
* LangChain
* Ollama
* Streamlit
* SQLite
* Python

## Setup
### Installation 
* Download and install Ollama from https://ollama.com/
* run commands: 
    ollama pull mistral
* pip install -r requirements.txt

## Directory structure
promptcache-playground/

â”œâ”€â”€ app.py                  # Streamlit UI

â”œâ”€â”€ prompt_runner.py        # Core logic (LLM, caching, logging)

â”œâ”€â”€ cache_log.csv           # Prompt logs (auto-created)

â”œâ”€â”€ .cache/                 # LangChain SQLite cache (auto-created)

## ğŸ› ï¸ How It Works
ğŸ” Prompt Caching via LangChain
Uses SQLiteCache from LangChain

When a prompt is run:

If cached: fetch from SQLite

If not: LLM is invoked, and response cached

LLM used: Ollama(model="mistral") (can be changed)



ğŸ•µï¸ HIT/MISS Detection
Uses latency threshold (100ms) to infer if cache was hit

Alternatively, can use direct DB checks with lookup() (see advanced notes)



ğŸ“ Logging
All prompt runs are logged in cache_log.csv

## â–¶ï¸ Running the App
Start your local LLM (e.g., Mistral): 

ollama run mistral

Launch the Streamlit UI:

streamlit run app.py

