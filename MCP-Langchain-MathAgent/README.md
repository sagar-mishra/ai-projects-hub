# ğŸ”¢ MCP-Langchain-MathAgent

An interactive AI-powered math assistant that uses **LangChain**, **Ollama**, and the **FastMCP** framework to dynamically reason about and call mathematical tools like `add`, `subtract`, and `divide` â€” all running **locally**. This project showcases how LLMs can use structured tools via **Model Context Protocol (MCP)** with zero external API dependencies.

---

## ğŸš€ What is MCP?

**Model Context Protocol (MCP)** is an open protocol that defines how tools (APIs, functions, etc.) are exposed to large language models in a structured way.

**FastMCP** is a lightweight Python framework for building MCP-compatible tool servers easily.

ğŸ”— Official GitHub: [https://github.com/jlowin/fastmcp](https://github.com/jlowin/fastmcp)  
ğŸŒ Website: [https://gofastmcp.com](https://gofastmcp.com)

---

## ğŸ“Œ Project Overview

| Feature                     | Description                                             |
|---------------------------- |---------------------------------------------------------|
| ğŸ§  LLM                      | Mistral 7B via [Ollama](https://ollama.com/)            |
| ğŸ”— Agent Framework          | [LangChain](https://www.langchain.com/)                 |
| ğŸ› ï¸ Tool Interface           | [FastMCP](https://github.com/jlowin/fastmcp)            |
| ğŸŒ UI                       | [Streamlit](https://streamlit.io/) Web Interface        |
| ğŸ”’ Privacy                  | 100% local â€” no cloud or API keys required              |

---

## ğŸ§  How It Works

This project shows how an LLM can reason about a question, discover tools dynamically, and call the correct one using **MCP**:

1. âœ… The MCP server exposes math tools (`add`, `multiply`, etc.)
2. âœ… The LangChain agent fetches available tools via `MCPClient`
3. âœ… The LLM (Mistral via Ollama) receives the tools + user prompt
4. âœ… LLM chooses the right tool and returns the answer
5. âœ… The answer is shown in the Streamlit UI

---

## ğŸ“ Folder Structure
MCP-Langchain-MathAgent/ <br>
â”œâ”€â”€ app.py # Streamlit UI <br>
â”œâ”€â”€ requirements.txt # Python dependencies <br>
â”‚   <br>
â”œâ”€â”€ client_agent/ <br>
â”‚ â”œâ”€â”€ init.py <br>
â”‚ â””â”€â”€ agent.py # LangChain agent with MCP client <br>
â”‚   <br>
â””â”€â”€ mcp_server/ <br>
â”œâ”€â”€ init.py <br>
â””â”€â”€ mcp_server.py # FastMCP server with math tools <br>

--- 

## Set Up Python Environment
conda create -n langchain_env python=3.10 -y <br>
conda activate langchain_env <br>
pip install -r requirements.txt <br>

<b>Install Ollama and Pull Model</b> <br>
Install Ollama: https://ollama.com/download <br>
Then pull Mistral: <br>
ollama pull mistral

## â–¶ï¸ Run the App
streamlit run app.py

This will:
<ul> 
    <li>ğŸ”„ Start the MCP server in the background</li>
    <li>âš™ï¸ Launch the Streamlit UI</li>
    <li>ğŸ§  Accept math queries like: What is 25 divided by 5?</li>
</ul>

ğŸ”„ Start the MCP server in the background

âš™ï¸ Launch the Streamlit UI

ğŸ§  Accept math queries like:
What is 25 divided by 5?

## ğŸ” Behind the Scenes (Workflow) 
<ol> 
<li> UI Input: User enters: "What is 25 divided by 5?" </li>
<li> Agent Fetches Tools: client.get_tools_summary() â†’ [add(), subtract(), divide(), ...] </li>
<li> LangChain Prompting: <br>
You are an intelligent math assistant.<br>
You can call tools when appropriate.<br>
Tools: add(a, b), subtract(a, b), ...<br>
User: What is 25 divided by 5?<br>
Assistant:<br></li>
<li>LLM Response: divide(25, 5) â†’ 5</li>
<li>Output: Streamlit shows result</li>
</ol>


